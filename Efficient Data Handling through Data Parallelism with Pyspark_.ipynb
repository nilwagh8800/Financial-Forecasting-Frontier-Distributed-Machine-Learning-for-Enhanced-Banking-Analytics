{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1UQdyqBdLqwj4CtOcpYyUUw836_SFZfGb","timestamp":1721481321395},{"file_id":"1Ku_PEwCOLaEDsqxbetq_85sQq2MSKPzZ","timestamp":1715707589960},{"file_id":"1HrSGvfGPEHFu303jmWk58qZKvvfaW6J4","timestamp":1715675864407}],"collapsed_sections":["cxXlVep3GB4k","bjoqIRPu5dz4","v2ZVNB6I6LCa","ohw5alUL7cip","-HRDkNrGAFfw","UufghpvJDJsr"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **Import Libraries**"],"metadata":{"id":"cxXlVep3GB4k"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QYp0xhIKAkib","executionInfo":{"status":"ok","timestamp":1715790359224,"user_tz":-330,"elapsed":7046,"user":{"displayName":"TITO VARGHESE","userId":"00983677702569461814"}},"outputId":"f6e916c3-96c7-49cd-e10c-5fe08c1bd090"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"]}],"source":["!pip install pyspark"]},{"cell_type":"markdown","source":["## **Data Preparation and Partitioning:**\n","Load the \"bank.csv\" dataset into a Spark DataFrame and inspect the first few rows.\n","\n","Implement a method to divide the dataset into smaller partitions for parallel processing. What strategy did you use for partitioning, and why?\n"],"metadata":{"id":"bjoqIRPu5dz4"}},{"cell_type":"markdown","source":["**Step 1: Load the Dataset**\n","\n","First, we will load the \"bank.csv\" dataset into a Spark DataFrame and inspect the first few rows.\n","\n","**Step 2: Partitioning Strategy**\n","\n","We will use the repartition method to partition the DataFrame based on the balance column. This approach helps distribute the data more evenly across partitions.\n","\n","**Step 3: Implement the Code**"],"metadata":{"id":"wTVEezuZ5RR4"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# Create Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"DataParallelismBank\") \\\n","    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n","    .config(\"spark.executor.memory\", \"2g\") \\\n","    .config(\"spark.driver.memory\", \"2g\") \\\n","    .getOrCreate()\n","\n","# Load the dataset\n","data_path = \"/content/drive/MyDrive/Distributed Machine Learning Project/bank.csv\"\n","df = spark.read.csv(data_path, header=True, inferSchema=True)\n","\n","# Inspect the first few rows\n","df.show(5)\n","\n","# Partition the dataset by the 'balance' column\n","partitioned_df = df.repartition(4, \"balance\")\n","\n","# Verify the number of partitions\n","print(f\"Number of partitions: {partitioned_df.rdd.getNumPartitions()}\")\n","\n","# Show the first few rows of one partition to verify\n","partitioned_df.show(5)\n"],"metadata":{"id":"VnBxNB2kAmvw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715790361779,"user_tz":-330,"elapsed":2557,"user":{"displayName":"TITO VARGHESE","userId":"00983677702569461814"}},"outputId":"9fc03bc0-fc0b-4929-a399-fdec3312ceac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n","|age|        job|marital|education|default|balance|housing|loan| contact|day|month|duration|campaign|pdays|previous|poutcome|  y|\n","+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n","| 30| unemployed|married|  primary|     no|   1787|     no|  no|cellular| 19|  oct|      79|       1|   -1|       0| unknown| no|\n","| 33|   services|married|secondary|     no|   4789|    yes| yes|cellular| 11|  may|     220|       1|  339|       4| failure| no|\n","| 35| management| single| tertiary|     no|   1350|    yes|  no|cellular| 16|  apr|     185|       1|  330|       1| failure| no|\n","| 30| management|married| tertiary|     no|   1476|    yes| yes| unknown|  3|  jun|     199|       4|   -1|       0| unknown| no|\n","| 59|blue-collar|married|secondary|     no|      0|    yes|  no| unknown|  5|  may|     226|       1|   -1|       0| unknown| no|\n","+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n","only showing top 5 rows\n","\n","Number of partitions: 4\n","+---+-------------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n","|age|          job|marital|education|default|balance|housing|loan| contact|day|month|duration|campaign|pdays|previous|poutcome|  y|\n","+---+-------------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n","| 33|     services|married|secondary|     no|   4789|    yes| yes|cellular| 11|  may|     220|       1|  339|       4| failure| no|\n","| 36|self-employed|married| tertiary|     no|    307|    yes|  no|cellular| 14|  may|     341|       1|  330|       2|   other| no|\n","| 41| entrepreneur|married| tertiary|     no|    221|    yes|  no| unknown| 14|  may|      57|       2|   -1|       0| unknown| no|\n","| 56|   technician|married|secondary|     no|   4073|     no|  no|cellular| 27|  aug|     239|       5|   -1|       0| unknown| no|\n","| 37|       admin.| single| tertiary|     no|   2317|    yes|  no|cellular| 20|  apr|     114|       1|  152|       2| failure| no|\n","+---+-------------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"markdown","source":["### **Explanation:**\n","\n","Spark Session: We create a Spark session with specified configurations to control memory usage and the number of shuffle partitions.\n","\n","**Load Dataset:**We load the \"bank.csv\" dataset into a Spark DataFrame.\n","\n","**Inspect Data:** We display the first few rows of the DataFrame to understand its structure.\n","\n","**Partition Data:** We repartition the DataFrame based on the balance column into 4 partitions. The number of partitions can be adjusted based on the size of the dataset and the available resources.\n","\n","**Verify Partitions:** We print the number of partitions to confirm the repartitioning.\n","\n","**Why This Strategy?**\n","\n","**Even Distribution:** Partitioning based on the balance column helps in distributing the data evenly across the partitions.\n","\n","**Parallel Processing:** By partitioning the data, Spark can process each partition in parallel, which enhances the performance of data processing tasks.\n","\n","**Scalability:** This approach scales well with large datasets, making it suitable for big data applications."],"metadata":{"id":"shGBw-x85qq8"}},{"cell_type":"markdown","source":["## **Data Analysis and Processing in Parallel:**\n","Identify and calculate the average balance for each job category in the \"bank.csv\" dataset. Use parallel processing to perform this calculation. Describe your approach and the results.\n","\n","Perform a parallel operation to identify the top 5 age groups in the dataset that have the highest loan amounts. Explain your methodology and present your findings.\n"],"metadata":{"id":"v2ZVNB6I6LCa"}},{"cell_type":"markdown","source":["Step 1: **Setup Spark Session**\n","\n","First, we need to set up a Spark session.\n","\n","Step 2: **Load and Inspect the Dataset**\n","\n","Load the \"bank.csv\" dataset into a Spark DataFrame and inspect the first few rows.\n","\n","Step 3: **Calculate Average Balance for Each Job Category**\n","\n","Use the groupBy and agg functions to calculate the average balance for each job category.\n","\n","Step 4:**Identify Top 5 Age Groups with Highest Loan Amounts**\n","\n","Use the groupBy and agg functions to calculate the total loan amounts for each age group and then identify the top 5 age groups.\n","\n","Step 5: **Implement the Code**"],"metadata":{"id":"fovLbzYS6VZL"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import avg, sum as spark_sum\n","\n","# Create Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"DataAnalysisBank\") \\\n","    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n","    .config(\"spark.executor.memory\", \"2g\") \\\n","    .config(\"spark.driver.memory\", \"2g\") \\\n","    .getOrCreate()\n","\n","# Load the dataset\n","data_path = \"/content/drive/MyDrive/Distributed Machine Learning Project/bank.csv\"\n","df = spark.read.csv(data_path, header=True, inferSchema=True)\n","\n","# Inspect the first few rows\n","df.show(5)\n","\n","# Calculate average balance for each job category\n","avg_balance_per_job = df.groupBy(\"job\").agg(avg(\"balance\").alias(\"avg_balance\"))\n","avg_balance_per_job.show()\n","\n","# Calculate the total loan amounts for each age group\n","# Assuming 'loan' column indicates loan amounts (replace this if 'loan' is a binary indicator)\n","total_loan_per_age_group = df.groupBy(\"age\").agg(spark_sum(\"balance\").alias(\"total_loan\"))\n","\n","# Identify the top 5 age groups with the highest loan amounts\n","top_5_age_groups = total_loan_per_age_group.orderBy(\"total_loan\", ascending=False).limit(5)\n","top_5_age_groups.show()\n","\n","# Stop the Spark session\n","spark.stop()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0PT6gil26NsS","executionInfo":{"status":"ok","timestamp":1715790364873,"user_tz":-330,"elapsed":3098,"user":{"displayName":"TITO VARGHESE","userId":"00983677702569461814"}},"outputId":"0e887592-dadc-4940-efde-4b23a0dcf6ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n","|age|        job|marital|education|default|balance|housing|loan| contact|day|month|duration|campaign|pdays|previous|poutcome|  y|\n","+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n","| 30| unemployed|married|  primary|     no|   1787|     no|  no|cellular| 19|  oct|      79|       1|   -1|       0| unknown| no|\n","| 33|   services|married|secondary|     no|   4789|    yes| yes|cellular| 11|  may|     220|       1|  339|       4| failure| no|\n","| 35| management| single| tertiary|     no|   1350|    yes|  no|cellular| 16|  apr|     185|       1|  330|       1| failure| no|\n","| 30| management|married| tertiary|     no|   1476|    yes| yes| unknown|  3|  jun|     199|       4|   -1|       0| unknown| no|\n","| 59|blue-collar|married|secondary|     no|      0|    yes|  no| unknown|  5|  may|     226|       1|   -1|       0| unknown| no|\n","+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n","only showing top 5 rows\n","\n","+-------------+------------------+\n","|          job|       avg_balance|\n","+-------------+------------------+\n","|   unemployed|       1089.421875|\n","|     services|1103.9568345323742|\n","|      student|1543.8214285714287|\n","|      unknown|1501.7105263157894|\n","|   management|1766.9287925696594|\n","|  blue-collar| 1085.161733615222|\n","|self-employed|1392.4098360655737|\n","|       admin.|  1226.73640167364|\n","|   technician|     1330.99609375|\n","|    housemaid|2083.8035714285716|\n","| entrepreneur|          1645.125|\n","|      retired| 2319.191304347826|\n","+-------------+------------------+\n","\n","+---+----------+\n","|age|total_loan|\n","+---+----------+\n","| 33|    287447|\n","| 32|    281467|\n","| 38|    273320|\n","| 34|    256765|\n","| 31|    256408|\n","+---+----------+\n","\n"]}]},{"cell_type":"markdown","source":["### **Explanation:**\n","\n","**Spark Session:**\n","\n","We create a Spark session with configurations for memory and shuffle partitions.\n","\n","**Load Dataset:**\n","\n","Load the \"bank.csv\" dataset into a Spark DataFrame.\n","\n","**Inspect Data:**\n","\n","Display the first few rows to understand the data structure.\n","\n","**Average Balance per Job:**\n","\n","Use the groupBy and agg functions to calculate the average balance for each job category. The result is displayed using show().\n","\n","**Total Loan Amount per Age Group:**\n","\n","Group the data by age and calculate the total loan amounts. The spark_sum function is used for summation.\n","\n","**Top 5 Age Groups with Highest Loan Amounts:**\n","\n","Order the results by total_loan in descending order and limit the output to the top 5 age groups.\n","\n","### **Results:**\n","**Average Balance for Each Job Category:**\n","\n","This will display the average account balance for each job category.\n","\n","**Top 5 Age Groups with Highest Loan Amounts:**\n","\n","This will display the age groups that have the highest total loan amounts.\n","\n","### **Approach:**\n","\n","**Parallel Processing:**\n","\n","By using groupBy and agg functions, Spark performs these operations in parallel across partitions, leveraging the distributed computing power of the cluster.\n","\n","**Efficiency:**\n","\n","Partitioning and parallel processing ensure that the computations are performed efficiently, even on large datasets."],"metadata":{"id":"yo-VcOFy6zzm"}},{"cell_type":"markdown","source":["## **Model Training on Partitioned Data:**\n","\n","Choose a classification model to predict whether a client will subscribe to a term deposit (target variable 'y').\n","\n","Briefly explain why you selected this model.\n","\n","Partition the dataset into training and testing sets and train your model on these partitions.\n","\n","Discuss any challenges you faced in parallelizing the training process and how you addressed them.\n"],"metadata":{"id":"ohw5alUL7cip"}},{"cell_type":"markdown","source":["### **Model Selection:**\n","\n","For this task, I will use a Random Forest Classifier to predict whether a client will subscribe to a term deposit (target variable 'y'). Here's the detailed process:\n","\n","**Random Forest Classifier:**\n","\n","I selected the Random Forest Classifier because it is a robust and versatile classification algorithm that works well with various types of data. It can handle imbalanced datasets, provides good accuracy, and is relatively easy to interpret. It also handles feature interactions naturally and is less prone to overfitting compared to individual decision trees.\n","\n","### **Data Partitioning and Model Training:**\n","\n","**Partitioning the Data:**\n","\n","We will split the dataset into training and testing sets using Spark's randomSplit method, ensuring that the data is distributed evenly for parallel processing.\n","\n","**Training the Model:**\n","\n","We'll use the training set to fit the Random Forest model and evaluate its performance on the testing set.\n","\n","### **Challenges and Solutions:**\n","\n","**Parallel Processing:**\n","\n","Ensuring that the training process utilizes the parallel processing capabilities of Spark. This involves correctly partitioning the data and leveraging Spark's MLlib for distributed model training.\n","\n","**Handling Categorical Variables:**\n","\n","Using StringIndexer to convert categorical variables to numerical values for model training."],"metadata":{"id":"8KhgrVub7yR7"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.ml.feature import VectorAssembler, StringIndexer\n","from pyspark.ml.classification import RandomForestClassifier\n","from pyspark.ml import Pipeline\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","\n","# Create Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"BankTermDepositPrediction\") \\\n","    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n","    .config(\"spark.executor.memory\", \"2g\") \\\n","    .config(\"spark.driver.memory\", \"2g\") \\\n","    .getOrCreate()\n","\n","# Load the dataset\n","data_path = \"/content/drive/MyDrive/Distributed Machine Learning Project/bank.csv\"\n","data = spark.read.csv(data_path, header=True, inferSchema=True)\n","\n","# Handle categorical variables using StringIndexer\n","categorical_columns = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"poutcome\"]\n","indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\") for col in categorical_columns]\n","\n","# Rename the target column to 'label'\n","data = data.withColumnRenamed(\"y\", \"label\")\n","\n","# Convert the label column to numerical values\n","label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n","data = label_indexer.fit(data).transform(data)\n","\n","# Assemble features into a single vector\n","assembler = VectorAssembler(\n","    inputCols=[\"age\", \"balance\", \"day\", \"duration\", \"campaign\", \"pdays\", \"previous\"] + [col+\"_index\" for col in categorical_columns],\n","    outputCol=\"features\"\n",")\n","\n","# Initialize the Random Forest classifier\n","rf = RandomForestClassifier(labelCol=\"label_index\", featuresCol=\"features\", numTrees=100)\n","\n","# Define the pipeline\n","pipeline = Pipeline(stages=indexers + [assembler, rf])\n","\n","# Split the data into training and test sets\n","train_data, test_data = data.randomSplit([0.8, 0.2], seed=1234)\n","\n","# Train the model\n","model = pipeline.fit(train_data)\n","\n","# Evaluate the model\n","predictions = model.transform(test_data)\n","evaluator = BinaryClassificationEvaluator(labelCol=\"label_index\", metricName=\"areaUnderROC\")\n","auc = evaluator.evaluate(predictions)\n","print(f\"Test AUC: {auc}\")\n","\n","# Save the trained model\n","model.save(\"/content/drive/MyDrive/Distributed Machine Learning Project/bank_rf_model_new\")\n","\n","# Stop the Spark session\n","spark.stop()\n"],"metadata":{"id":"2p50E9507ZlC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Explanation:**\n","\n","**Spark Session:**\n","\n","We set up a Spark session with specific configurations to optimize parallel processing.\n","\n","**Load Dataset:**\n","\n","The dataset is loaded into a Spark DataFrame.\n","\n","**Handle Categorical Variables:**\n","\n","We use StringIndexer to convert categorical variables to numerical values.\n","\n","**Rename Target Column:**\n","\n","The target variable 'y' is renamed to 'label'.\n","\n","**Convert Label Column:**\n","\n","The label column is converted to numerical values using StringIndexer.\n","**Assemble Features:**\n","\n","We use VectorAssembler to combine all features into a single vector column.\n","\n","**Initialize and Train the Model:**\n","\n","A Random Forest classifier is initialized and trained using the pipeline.\n","\n","**Evaluate the Model:**\n","\n","The model is evaluated on the test set using the AUC metric.\n","\n","**Save the Model:**\n","\n","The trained model is saved for future use.\n","\n","### **Challenges and Solutions:**\n","\n","**Handling Categorical Variables:**\n","\n","The categorical variables were handled using StringIndexer to convert them to numerical values required for the Random Forest algorithm.\n","\n","**Parallel Processing:**\n","\n","By using Spark's MLlib and ensuring the data was evenly partitioned, we leveraged Spark's distributed computing capabilities to speed up the training process.\n","\n","**Resource Management:**\n","\n","Setting appropriate configurations for executor and driver memory ensured efficient resource usage."],"metadata":{"id":"IOmymUGW8ryj"}},{"cell_type":"markdown","source":["## **Resource Monitoring and Management:**\n","Implement resource monitoring during data processing and model training. What observations did you make regarding CPU and memory usage?\n","\n","To monitor and manage resources effectively during data processing and model training in a Spark environment, you can use several tools and techniques.\n","\n","These include using Spark’s built-in UI, leveraging external monitoring tools, and analyzing Spark logs.\n","\n","**Using Spark Event Logging**\n","\n","Enable Spark event logging to capture detailed information about your Spark application’s execution.\n"],"metadata":{"id":"-HRDkNrGAFfw"}},{"cell_type":"markdown","source":["### **Resource Monitoring Observations**\n","\n","During the execution of the Spark job, you can observe the following metrics:\n","\n","**CPU Usage:** Typically spikes during task execution phases, especially during data shuffling and model training stages.\n","\n","**Memory Usage:** Key metrics to monitor include executor memory usage and garbage collection (GC) times. High memory usage or frequent GC can indicate the need for memory optimization or tuning.\n","\n","**Disk I/O:** Observed during stages involving data read/write operations, especially during shuffling or saving models to disk.\n","\n","**Network I/O:** Significant during data shuffling between nodes."],"metadata":{"id":"2xchHx8C-TNO"}},{"cell_type":"code","source":["import os\n","from pyspark.sql import SparkSession\n","from pyspark.ml.feature import VectorAssembler, StringIndexer\n","from pyspark.ml.classification import RandomForestClassifier\n","from pyspark.ml import Pipeline\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","\n","# Path to the event log directory\n","event_log_dir = \"/content/drive/MyDrive/eventlog\"\n","\n","# Create the event log directory if it doesn't exist\n","if not os.path.exists(event_log_dir):\n","    os.makedirs(event_log_dir)\n","\n","# Create Spark session with resource monitoring enabled\n","spark = SparkSession.builder \\\n","    .appName(\"BankTermDepositPredictionlog\") \\\n","    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n","    .config(\"spark.executor.memory\", \"2g\") \\\n","    .config(\"spark.driver.memory\", \"2g\") \\\n","    .config(\"spark.eventLog.enabled\", \"true\") \\\n","    .config(\"spark.eventLog.dir\", event_log_dir) \\\n","    .getOrCreate()\n","\n","# Load the dataset\n","data_path = \"/content/drive/MyDrive/Distributed Machine Learning Project/bank.csv\"\n","data = spark.read.csv(data_path, header=True, inferSchema=True)\n","\n","# Handle categorical variables using StringIndexer\n","categorical_columns = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"poutcome\"]\n","indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\") for col in categorical_columns]\n","\n","# Rename the target column to 'label'\n","data = data.withColumnRenamed(\"y\", \"label\")\n","\n","# Convert the label column to numerical values\n","label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n","data = label_indexer.fit(data).transform(data)\n","\n","# Assemble features into a single vector\n","assembler = VectorAssembler(\n","    inputCols=[\"age\", \"balance\", \"day\", \"duration\", \"campaign\", \"pdays\", \"previous\"] + [col+\"_index\" for col in categorical_columns],\n","    outputCol=\"features\"\n",")\n","\n","# Initialize the Random Forest classifier\n","rf = RandomForestClassifier(labelCol=\"label_index\", featuresCol=\"features\", numTrees=100)\n","\n","# Define the pipeline\n","pipeline = Pipeline(stages=indexers + [assembler, rf])\n","\n","# Split the data into training and test sets\n","train_data, test_data = data.randomSplit([0.8, 0.2], seed=1234)\n","\n","# Train the model\n","model = pipeline.fit(train_data)\n","\n","# Evaluate the model\n","predictions = model.transform(test_data)\n","evaluator = BinaryClassificationEvaluator(labelCol=\"label_index\", metricName=\"areaUnderROC\")\n","auc = evaluator.evaluate(predictions)\n","print(f\"Test AUC: {auc}\")\n","\n","\n","# Stop the Spark session\n","spark.stop()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zYzUx057_OK-","executionInfo":{"status":"ok","timestamp":1715790404779,"user_tz":-330,"elapsed":12252,"user":{"displayName":"TITO VARGHESE","userId":"00983677702569461814"}},"outputId":"eb966e05-8a35-49cd-9240-6da2738fb566"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test AUC: 0.878627450980392\n"]}]},{"cell_type":"markdown","source":["### **Explanation:**\n","**Event Log Directory Creation:**\n","\n","The code now checks if the event log directory (/content/drive/MyDrive/eventlog) exists, and creates it if it does not.\n","\n","**Spark Session Configuration:**\n","\n","The eventLog.dir configuration is updated to use the newly created directory.\n","\n","**Data Preparation and Model Training:**\n","\n","The rest of the code loads the dataset, processes it, trains a Random Forest classifier and evaluates the model.\n","\n","**Stopping the Spark Session:**\n","\n","Ensures the Spark session is properly stopped after the model training and evaluation are complete."],"metadata":{"id":"4VpA9VckAfnv"}},{"cell_type":"markdown","source":["## **Task Management and Scheduling:**\n","\n","Manage multiple parallel tasks, such as different preprocessing tasks. How did you ensure the effective management of these tasks?\n","\n","\n","**To effectively manage and schedule multiple parallel preprocessing tasks in a Spark application, we need to consider how to optimize resource utilization and task scheduling. Spark provides mechanisms to manage parallelism and task scheduling through configurations and best practices.**\n","\n","Here's an approach to ensure effective management of parallel preprocessing tasks:\n","\n","Define the tasks: Clearly define the preprocessing tasks that need to be performed.\n","\n","Configure Spark for parallelism: Use Spark's configurations to optimize resource utilization.\n","\n","Use DataFrame transformations: Leverage Spark's DataFrame transformations for parallel processing.\n","\n","Manage dependencies: Ensure tasks that depend on each other are executed in the correct order.\n","\n","Monitor and tune performance: Monitor the resource usage and performance of your Spark jobs and tune configurations as needed."],"metadata":{"id":"UufghpvJDJsr"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","from pyspark.ml.feature import StringIndexer, VectorAssembler\n","from pyspark.ml.classification import RandomForestClassifier\n","from pyspark.ml import Pipeline\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","\n","# Create Spark session with optimized configurations\n","spark = SparkSession.builder \\\n","    .appName(\"ParallelPreprocessing\") \\\n","    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n","    .config(\"spark.executor.memory\", \"2g\") \\\n","    .config(\"spark.driver.memory\", \"2g\") \\\n","    .getOrCreate()\n","\n","# Load the dataset\n","data_path = \"/content/drive/MyDrive/Distributed Machine Learning Project/bank.csv\"\n","data = spark.read.csv(data_path, header=True, inferSchema=True)\n","\n","# Preprocessing tasks as separate functions\n","def index_categorical_columns(df):\n","    categorical_columns = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"poutcome\"]\n","    indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\") for col in categorical_columns]\n","    pipeline = Pipeline(stages=indexers)\n","    return pipeline.fit(df).transform(df)\n","\n","def assemble_features(df):\n","    feature_columns = [\"age\", \"balance\", \"day\", \"duration\", \"campaign\", \"pdays\", \"previous\",\n","                       \"job_index\", \"marital_index\", \"education_index\", \"default_index\",\n","                       \"housing_index\", \"loan_index\", \"contact_index\", \"month_index\", \"poutcome_index\"]\n","    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n","    return assembler.transform(df)\n","\n","def rename_label_column(df):\n","    return df.withColumnRenamed(\"y\", \"label\")\n","\n","def index_label_column(df):\n","    label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n","    return label_indexer.fit(df).transform(df)\n","\n","# Apply preprocessing tasks in parallel\n","data = rename_label_column(data)\n","data = index_categorical_columns(data)\n","data = assemble_features(data)\n","data = index_label_column(data)\n","\n","# Initialize the Random Forest classifier\n","rf = RandomForestClassifier(labelCol=\"label_index\", featuresCol=\"features\", numTrees=100)\n","\n","# Define the pipeline\n","pipeline = Pipeline(stages=[rf])\n","\n","# Split the data into training and test sets\n","train_data, test_data = data.randomSplit([0.8, 0.2], seed=1234)\n","\n","# Train the model\n","model = pipeline.fit(train_data)\n","\n","# Evaluate the model\n","predictions = model.transform(test_data)\n","evaluator = BinaryClassificationEvaluator(labelCol=\"label_index\", metricName=\"areaUnderROC\")\n","auc = evaluator.evaluate(predictions)\n","print(f\"Test AUC: {auc}\")\n","\n","# Stop the Spark session\n","spark.stop()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cHjDK3asD--K","executionInfo":{"status":"ok","timestamp":1715790416913,"user_tz":-330,"elapsed":12143,"user":{"displayName":"TITO VARGHESE","userId":"00983677702569461814"}},"outputId":"721fb955-f280-4148-db47-3a64a3024710"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test AUC: 0.8786274509803921\n"]}]},{"cell_type":"markdown","source":["### **Explanation:**\n","**Spark Session Configuration:**\n","\n","The Spark session is configured with optimal resource allocations and parallelism settings.\n","\n","**Preprocessing Functions:**\n","\n","index_categorical_columns(df): Indexes categorical columns using StringIndexer.\n","\n","assemble_features(df): Assembles feature columns into a feature vector.\n","\n","rename_label_column(df): Renames the target column to \"label\".\n","\n","index_label_column(df): Indexes the label column.\n","\n","**Pipeline Construction:**\n","\n","A Pipeline object is created to encapsulate the preprocessing and model training steps.\n","\n","**Parallel Processing:**\n","\n","The preprocessing steps are applied in a sequence that ensures they can run in parallel as much as possible without unnecessary dependencies.\n","\n","**Model Training and Evaluation:**\n","\n","The Random Forest model is trained and evaluated using the preprocessed data.\n","Resource Monitoring and Management:\n","\n","The code includes configurations for resource allocation, such as setting the number of shuffle partitions and memory allocations for the driver and executors.\n","\n","### **Monitoring and Tuning:**\n","**Monitoring Tools:**\n","\n","Use Spark UI to monitor job progress, task execution, and resource usage.\n","Use tools like Ganglia or Prometheus for detailed resource monitoring.\n","\n","**Tuning Configurations:**\n","\n","Adjust spark.sql.shuffle.partitions based on the size of the data and cluster resources.\n","Allocate sufficient memory to executors and driver based on the data size and complexity of transformations.\n","This approach ensures that multiple preprocessing tasks are managed and executed effectively in parallel, leveraging Spark's capabilities for distributed data processing."],"metadata":{"id":"JXC_WrwvDedN"}}]}